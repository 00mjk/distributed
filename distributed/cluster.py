from __future__ import print_function, division, absolute_import

import paramiko
from time import sleep
import socket
import os

from toolz import merge

# These are handy for creating colorful terminal output to enhance readability
# of the output generated by dcluster.
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def start_center(logdir, center_addr, center_port):

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(center_addr, timeout = 20)

    cmd = 'dcenter --host {addr} --port {port}'.format(
        addr = center_addr, port = center_port, logdir = logdir)

    # Optionally re-direct stdout and stderr to a logfile
    if logdir is not None:
        cmd = 'mkdir -p {logdir} && ' + cmd
        cmd += '&> {logdir}/dcenter_{addr}:{port}.log\n'.format(
            addr = center_addr, port = center_port, logdir = logdir)

    # Run the command
    stdin, stdout, stderr = ssh.exec_command(cmd, get_pty=True)
    return {'address': center_addr, 'port': center_port, 'client': ssh, 'stdout': stdout, 'stderr': stderr}


def start_worker(logdir, center_addr, center_port, worker_addr, workers_per_node, cpus_per_worker):

    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(worker_addr, timeout = 20)

    # Pick a random port for the worker.  This prevents contention over a single port,
    # which may occasionally cause workers to fail to register with the center node.
    import random
    worker_port = random.randint(10000, 20000)

    cmd = 'dworker {center_addr}:{center_port} --host {worker_addr} --port {worker_port}'.format(
        center_addr = center_addr, center_port = center_port,
        worker_addr = worker_addr, worker_port = worker_port,
        logdir = logdir)

    # (Optionally) add the ncores argument
    if cpus_per_worker is not None:
        cmd += ' --ncores {ncpu}\n'.format(ncpu = cpus_per_worker)

    # Optionally redirect stdout and stderr to a logfile
    if logdir is not None:
        cmd = 'mkdir -p {logdir} && ' + cmd
        cmd += '&> {logdir}/dcenter_{addr}:{port}.log\n'.format(
            addr = worker_addr, port = worker_port, logdir = logdir)

    # Run the command
    stdin, stdout, stderr = ssh.exec_command(cmd, get_pty=True)
    return {'address': worker_addr, 'port': worker_port, 'client': ssh, 'stdout': stdout, 'stderr': stderr}


class Cluster(object):
    def __init__(self, center_addr, center_port, worker_addrs, workers_per_node = 1, cpus_per_worker = None, logdir = None):

        self.center_addr = center_addr
        self.center_port = center_port
        self.workers_per_node = workers_per_node
        self.cpus_per_worker = cpus_per_worker

        # Generate a universal timestamp to use for log files
        import datetime
        if logdir is not None:
            logdir = os.path.join(logdir, "dcluster_" + datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S"))
            print(bcolors.WARNING + 'Output will be redirected to logfiles stored locally on individual woker nodes under "{logdir}".'.format(logdir=logdir) + bcolors.ENDC)
        self.logdir = logdir

        # Start the center node
        self.center = merge(start_center(logdir, center_addr, center_port), {'address': center_addr})

        # Start worker nodes
        self.workers = []
        for addr in worker_addrs:
            self.add_worker(addr)

    def monitor_remote_processes(self):

        # Set up channel timeouts (which we rely on below to make readline()
        # non-blocking. Also set up some nicely formatted output labels we can
        # prepend to each line of output, and create a 'status' key to keep
        # track of jobs that terminate prematurely.
        self.center['label'] = (bcolors.BOLD +
                                'center {addr}:{port}'.format(addr = self.center['address'],
                                                               port = self.center['port']) +
                                bcolors.ENDC)
        self.center['status'] = 'running'
        self.center['stdout'].channel.settimeout(0.1)
        self.center['stderr'].channel.settimeout(0.1)

        for worker in self.workers:
            worker['label'] = 'worker {addr}:{port} {{{worker_id}}}'.format(worker_id = worker['worker_id'],
                                                                            addr = worker['address'],
                                                                            port = worker['port'])
            worker['status'] = 'running'
            worker['stdout'].channel.settimeout(0.1)
            worker['stderr'].channel.settimeout(0.1)

        # Form a list containing all processes, since we treat them equally from here on out.
        all_processes = [self.center] + self.workers

        try:
            while(1):
                for process in all_processes:

                    if process['status'] == 'finished':
                        continue

                    if process['stdout'].channel.exit_status_ready():
                        exit_status = process['stdout'].channel.recv_exit_status()
                        print('[ ' + process['label'] + ' ] : ' + bcolors.FAIL +
                              "remote process exited with exit status " + str(exit_status) + bcolors.ENDC)
                        process['status'] = 'finished'

                    # Read stdout stream, time out if necessary.
                    try:
                        line = process['stdout'].readline()
                        while len(line) > 0:    # Loops until a timout exception occurs
                            print('[ {label} ] : {output}'.format(label = process['label'],
                                                                  output = line.rstrip()))
                            line = process['stdout'].readline()

                    except paramiko.buffered_pipe.PipeTimeout:
                        continue
                    except socket.timeout:
                        continue

                    # Read stderr stream, time out if necessary
                    try:
                        line = process['stderr'].readline()
                        while len(line) > 0:
                            print('[ {label} ] : ' +
                                  bcolors.WARNING +
                                  ' {output}'.format(label = process['label'],
                                                     output = line.rstrip()) +
                                  bcolors.ENDC)
                            line = process['stderr'].readline()

                    except paramiko.buffered_pipe.PipeTimeout:
                        continue
                    except socket.timeout:
                        continue

                # Kill some time and free up CPU before starting the next sweep
                # through the processes.
                sleep(1)

                # end while()

        except KeyboardInterrupt:
            pass   # Return execution to the calling process

    def add_worker(self, address):
        self.workers += [ merge(start_worker(self.logdir, self.center_addr, self.center_port, address,
                                             self.workers_per_node, self.cpus_per_worker),
                                {'address': address, 'worker_id': worker_id})
                          for worker_id in range(self.workers_per_node)]

    def shutdown(self):

        # Close down sockets
        for d in self.workers:
            d['stdout'].channel.close()
            d['stderr'].channel.close()

        self.center['stdout'].channel.close()
        self.center['stderr'].channel.close()

        # Close down ssh connections
        for d in self.workers:
            d['client'].close()  # Calling this multiple times on the same connection seems ok.

        self.center['client'].close()

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

from __future__ import print_function, division, absolute_import

import paramiko
from time import sleep
import socket
import os

try:
    from queue import Queue
except ImportError:  # Python 2.7 fix
    from Queue import Queue

from threading import Thread

from toolz import merge

# These are handy for creating colorful terminal output to enhance readability
# of the output generated by dcluster.
class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def async_ssh(cmd_dict):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(cmd_dict['address'], timeout = 20)

    # Execute the command, and grab file handles for stdout and stderr. Note
    # that we run the command using the user's default shell, but force it to
    # run in an interactive login shell, which hopefully ensures that all of the
    # user's normal environment variables (via the dot files) have been loaded
    # before the command is run. This should help to ensure that important
    # aspects of the environment like PATH and PYTHONPATH are configured.

    print('[ {label} ] : {cmd}'.format(label = cmd_dict['label'],
                                       cmd = cmd_dict['cmd']))
    stdin, stdout, stderr = ssh.exec_command('$SHELL -i -l -c \'' + cmd_dict['cmd'] + '\'', get_pty = True)

    # Set up channel timeouts (which we rely on below to make readline()
    # non-blocking.
    stdout.channel.settimeout(0.1)
    stderr.channel.settimeout(0.1)

    # Wait for a message on the input_queue. Any message received signals this
    # thread to shut itself down.
    while(cmd_dict['input_queue'].empty()):

        # Read stdout stream, time out if necessary.
        try:
            line = stdout.readline()
            while len(line) > 0:    # Loops until a timout exception occurs
                cmd_dict['output_queue'].put('[ {label} ] : {output}'.format(label = cmd_dict['label'],
                                                                             output = line.rstrip()))
                line = stdout.readline()

        except paramiko.buffered_pipe.PipeTimeout:
            continue
        except socket.timeout:
            continue

        # Read stderr stream, time out if necessary
        try:
            line = stderr.readline()
            while len(line) > 0:
                cmd_dict['output_queue'].put('[ {label} ] : '.format(label = cmd_dict['label']) +
                                             bcolors.FAIL + '{output}'.format(output = line.rstrip()) + bcolors.ENDC)
                line = stderr.readline()

        except paramiko.buffered_pipe.PipeTimeout:
            continue
        except socket.timeout:
            continue

        # Check to see if the process has exited. If it has, we let this thread
        # terminate.
        if stdout.channel.exit_status_ready():
            exit_status = stdout.channel.recv_exit_status()
            cmd_dict['output_queue'].put('[ {label} ] : '.format(label = cmd_dict['label']) +
                                         bcolors.FAIL +
                                         "remote process exited with exit status " +
                                         str(exit_status) + bcolors.ENDC)
            break

        # Kill some time so that this thread does not hog the CPU.
        sleep(2.0)

    # end while()

    # Shutdown the channel, and close the SSH connection
    stdout.channel.close()
    stderr.channel.close()
    ssh.close()


def start_center(logdir, center_addr, center_port):

    cmd = 'dcenter --host {addr} --port {port}'.format(
        addr = center_addr, port = center_port, logdir = logdir)

    # Optionally re-direct stdout and stderr to a logfile
    if logdir is not None:
        cmd = 'mkdir -p {logdir} && ' + cmd
        cmd += '&> {logdir}/dcenter_{addr}:{port}.log'.format(
            addr = center_addr, port = center_port, logdir = logdir)

    # Format output labels we can prepend to each line of output, and create
    # a 'status' key to keep track of jobs that terminate prematurely.
    label = (bcolors.BOLD +
             'center {addr}:{port}'.format(addr = center_addr,
                                           port = center_port) +
             bcolors.ENDC)

    # Create a command dictionary, which contains everything we need to run and
    # interact with this command.
    input_queue = Queue()
    output_queue = Queue()
    cmd_dict = {'cmd': cmd, 'label': label, 'address': center_addr, 'port': center_port,
                'input_queue': input_queue, 'output_queue': output_queue}

    # Start the thread
    thread = Thread(target=async_ssh, args=[cmd_dict])
    thread.start()

    return merge(cmd_dict, {'thread': thread})

def start_worker(logdir, center_addr, center_port, worker_addr,
                 worker_id, workers_per_node, cpus_per_worker):

    # Pick a random port for the worker.  This prevents contention over a single port,
    # which may occasionally cause workers to fail to register with the center node.
    import random
    worker_port = random.randint(10000, 20000)

    cmd = 'dworker {center_addr}:{center_port} --host {worker_addr} --port {worker_port}'.format(
        center_addr = center_addr, center_port = center_port,
        worker_addr = worker_addr, worker_port = worker_port,
        logdir = logdir)

    # (Optionally) add the ncores argument
    if cpus_per_worker is not None:
        cmd += ' --ncores {ncpu}'.format(ncpu = cpus_per_worker)

    # Optionally redirect stdout and stderr to a logfile
    if logdir is not None:
        cmd = 'mkdir -p {logdir} && ' + cmd
        cmd += '&> {logdir}/dcenter_{addr}:{port}.log'.format(
            addr = worker_addr, port = worker_port, logdir = logdir)

    label = 'worker {addr}:{port} {{{worker_id}}}'.format(worker_id = worker_id,
                                                          addr = worker_addr,
                                                          port = worker_port)

    # Create a command dictionary, which contains everything we need to run and
    # interact with this command.
    input_queue = Queue()
    output_queue = Queue()
    cmd_dict = {'cmd': cmd, 'label': label, 'address': worker_addr, 'port': worker_port,
                'input_queue': input_queue, 'output_queue': output_queue}

    # Start the thread
    thread = Thread(target=async_ssh, args=[cmd_dict])
    thread.start()

    return merge(cmd_dict, {'thread': thread})



class Cluster(object):
    def __init__(self, center_addr, center_port, worker_addrs, workers_per_node = 1, cpus_per_worker = None, logdir = None):

        self.center_addr = center_addr
        self.center_port = center_port
        self.workers_per_node = workers_per_node
        self.cpus_per_worker = cpus_per_worker

        # Generate a universal timestamp to use for log files
        import datetime
        if logdir is not None:
            logdir = os.path.join(logdir, "dcluster_" + datetime.datetime.now().strftime("%Y-%m-%d_%H:%M:%S"))
            print(bcolors.WARNING + 'Output will be redirected to logfiles stored locally on individual woker nodes under "{logdir}".'.format(logdir=logdir) + bcolors.ENDC)
        self.logdir = logdir

        # Keep track of all running threads
        self.threads = []

        # Start the center node
        self.center = start_center(logdir, center_addr, center_port)

        # Start worker nodes
        self.workers = []
        for addr in worker_addrs:
            self.add_worker(addr)

    def monitor_remote_processes(self):

        # Form a list containing all processes, since we treat them equally from here on out.
        all_processes = [self.center] + self.workers

        try:
            while(1):
                for process in all_processes:
                    while not process['output_queue'].empty():
                        print(process['output_queue'].get())

                # Kill some time and free up CPU before starting the next sweep
                # through the processes.
                sleep(1.0)

            # end while(1)

        except KeyboardInterrupt:
            pass   # Return execution to the calling process

    def add_worker(self, address):
        for worker_id in range(self.workers_per_node):
            worker = merge( start_worker(self.logdir,
                                         self.center_addr,
                                         self.center_port,
                                         address,
                                         worker_id,
                                         self.workers_per_node,
                                         self.cpus_per_worker),
                            {'worker_id': worker_id})
            self.workers.append(worker)

    def shutdown(self):
        all_processes = [self.center] + self.workers

        for process in all_processes:
            process['input_queue'].put('shutdown')
            process['thread'].join()

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()
